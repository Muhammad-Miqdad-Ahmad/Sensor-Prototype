ST Edge AI Core v2.2.0-20266 2adc00962
Created date          : 2026-02-17 23:56:14
Parameters            : generate --target stm32l4 --name harsh_detection_prototype -m /home/cyber-surge/Work/Uni/Final Year Project /Sensor-Prototype/accelerometer/model.tflite --compression none --verbosity 1 --workspace /tmp/mxAI_workspace107676864688424771534423506009345 --output /home/cyber-surge/.stm32cubemx/harsh_detection_prototype_output

Exec/report summary (generate)
----------------------------------------------------------------------------------------------------------------------------
model file         :   /home/cyber-surge/Work/Uni/Final Year Project /Sensor-Prototype/accelerometer/model.tflite           
type               :   tflite                                                                                               
c_name             :   harsh_detection_prototype                                                                            
compression        :   none                                                                                                 
options            :   allocate-inputs, allocate-outputs                                                                    
optimization       :   balanced                                                                                             
target/series      :   stm32l4                                                                                              
workspace dir      :   /tmp/mxAI_workspace107676864688424771534423506009345                                                 
output dir         :   /home/cyber-surge/.stm32cubemx/harsh_detection_prototype_output                                      
model_fmt          :   ss/sa per channel                                                                                    
model_name         :   model                                                                                                
model_hash         :   0x0567c223e09c17f2e1f5f854ee6bc063                                                                   
params #           :   11,493 items (11.71 KiB)                                                                             
----------------------------------------------------------------------------------------------------------------------------
input 1/1          :   'serving_default_input_layer0', int8(1x40x6), 240 Bytes, QLinear(0.119718373,25,int8), activations   
output 1/1         :   'nl_16', int8(1x5), 5 Bytes, QLinear(0.003906250,-128,int8), activations                             
macc               :   137,904                                                                                              
weights (ro)       :   11,988 B (11.71 KiB) (1 segment) / -33,984(-73.9%) vs float model                                    
activations (rw)   :   8,096 B (7.91 KiB) (1 segment) *                                                                     
ram (total)        :   8,096 B (7.91 KiB) = 8,096 + 0 + 0                                                                   
----------------------------------------------------------------------------------------------------------------------------
(*) 'input'/'output' buffers are allocated in the activations buffer

Model name - model
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
m_id   layer (type,original)                      oshape                param/size         macc                   connected to   | c_size          c_macc            c_type                 
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
0      serving_default_input_layer0 (Input, )     [b:1,h:40,c:6]                                                                 |                                   
       reshape_0 (Reshape, EXPAND_DIMS)           [b:1,h:1,w:40,c:6]                              serving_default_input_layer0   |                                   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
1      conv2d_1 (Conv2D, CONV_2D)                 [b:1,h:1,w:38,c:32]   608/704          21,920                      reshape_0   |                                   Conv2D_[0]             
       nl_1_nl (Nonlinearity, CONV_2D)            [b:1,h:1,w:38,c:32]                     1,216                       conv2d_1   |                 -1,216(-100.0%)   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
2      tfl_pseudo_qconst9 (Placeholder, )         [b:32]                32/32                                                    |                 +1,216(+100.0%)   Eltwise/mul_[1]        
       eltwise_2 (Eltwise, MUL)                   [b:1,h:1,w:38,c:32]                     1,216                        nl_1_nl   |                 -1,216(-100.0%)   
                                                                                                            tfl_pseudo_qconst9   | 
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
3      tfl_pseudo_qconst8 (Placeholder, )         [b:32]                32/32                                                    |                 +1,216(+100.0%)   Eltwise/add_[2]        
       eltwise_3 (Eltwise, ADD)                   [b:1,h:1,w:38,c:32]                     1,216                      eltwise_2   |                 -1,216(-100.0%)   
                                                                                                            tfl_pseudo_qconst8   | 
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
4      reshape_4 (Reshape, RESHAPE)               [b:1,h:38,c:32]                                                    eltwise_3   |                                   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
5      reshape_5 (Reshape, EXPAND_DIMS)           [b:1,h:1,w:38,c:32]                                                reshape_4   |                                   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
6      pool_6 (Pool, MAX_POOL_2D)                 [b:1,h:1,w:19,c:32]                     1,216                      reshape_5   |                                   Pool_[3]               
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
7      reshape_7 (Reshape, RESHAPE)               [b:1,h:19,c:32]                                                       pool_6   |                                   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
8      reshape_8 (Reshape, EXPAND_DIMS)           [b:1,h:1,w:19,c:32]                                                reshape_7   |                                   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
9      conv2d_9 (Conv2D, CONV_2D)                 [b:1,h:1,w:17,c:64]   6,208/6,400     104,512                      reshape_8   |                                   Conv2D_[4]             
       nl_9_nl (Nonlinearity, CONV_2D)            [b:1,h:1,w:17,c:64]                     1,088                       conv2d_9   |                 -1,088(-100.0%)   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
10     tfl_pseudo_qconst5 (Placeholder, )         [b:64]                64/64                                                    |                 +1,088(+100.0%)   Eltwise/mul_[5]        
       eltwise_10 (Eltwise, MUL)                  [b:1,h:1,w:17,c:64]                     1,088                        nl_9_nl   |                 -1,088(-100.0%)   
                                                                                                            tfl_pseudo_qconst5   | 
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
11     tfl_pseudo_qconst4 (Placeholder, )         [b:64]                64/64                                                    |                 +1,088(+100.0%)   Eltwise/add_[6]        
       eltwise_11 (Eltwise, ADD)                  [b:1,h:1,w:17,c:64]                     1,088                     eltwise_10   |                 -1,088(-100.0%)   
                                                                                                            tfl_pseudo_qconst4   | 
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
12     reshape_12 (Reshape, RESHAPE)              [b:1,h:17,c:64]                                                   eltwise_11   |                                   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
13     pool_13 (Pool, MEAN)                       [b:1,h:1,c:64]                          1,088                     reshape_12   |                                   Pool_[7]               
       reshape_13_reshape (Reshape, MEAN)         [b:1,c:64]                                                           pool_13   |                                   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
14     tfl_pseudo_qconst3 (Placeholder, )         [b:64,c:64]           4,096/4,096                                              | +256(+6.2%)     +4,160(+100.0%)   Dense_[8]              
       tfl_pseudo_qconst2 (Placeholder, )         [b:64]                64/256                                                   | -256(-100.0%)                     
       gemm_14 (Gemm, FULLY_CONNECTED)            [b:1,c:64]                              4,160             reshape_13_reshape   |                 -4,160(-100.0%)   
                                                                                                            tfl_pseudo_qconst3   | 
                                                                                                            tfl_pseudo_qconst2   | 
       nl_14_nl (Nonlinearity, FULLY_CONNECTED)   [b:1,c:64]                                 64                        gemm_14   |                 -64(-100.0%)      
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
15     tfl_pseudo_qconst1 (Placeholder, )         [b:5,c:64]            320/320                                                  | +20(+6.2%)      +325(+100.0%)     Dense_[9]              
       tfl_pseudo_qconst (Placeholder, )          [b:5]                 5/20                                                     | -20(-100.0%)                      
       gemm_15 (Gemm, FULLY_CONNECTED)            [b:1,c:5]                                 325                       nl_14_nl   |                 -325(-100.0%)     
                                                                                                            tfl_pseudo_qconst1   | 
                                                                                                             tfl_pseudo_qconst   | 
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
16     nl_16 (Nonlinearity, SOFTMAX)              [b:1,c:5]                                  75                        gemm_15   |                                   Nonlinearity_[o][10]   
------ ------------------------------------------ --------------------- ------------- --------- ------------------------------ --- --------------- ----------------- ---------------------- 
model/c-model: macc=140,272/137,904 -2,368(-1.7%) weights=11,988/11,988  activations=--/8,096 io=--/0



Generated C-graph summary
------------------------------------------------------------------------------------------------------------------------
model name            : model
c-name                : harsh_detection_prototype
c-node #              : 11
c-array #             : 29
activations size      : 8096 (1 segment)
weights size          : 11988 (1 segment)
macc                  : 137904
inputs                : ['serving_default_input_layer0_output']
outputs               : ['nl_16_output']

C-Arrays (29)
------ ------------------------------------- ----------- ------------------------- ----------- --------- 
c_id   name (*_array)                        item/size   domain/mem-pool           c-type      comment   
------ ------------------------------------- ----------- ------------------------- ----------- --------- 
0      conv2d_1_bias                         32/128      weights/weights           const s32             
1      conv2d_1_output                       1216/1216   activations/**default**   s8                    
2      conv2d_1_scratch0                     1672/1672   activations/**default**   s8                    
3      conv2d_1_weights                      576/576     weights/weights           const s8              
4      conv2d_9_bias                         64/256      weights/weights           const s32             
5      conv2d_9_output                       1088/1088   activations/**default**   s8                    
6      conv2d_9_scratch0                     6400/6400   activations/**default**   s8                    
7      conv2d_9_weights                      6144/6144   weights/weights           const s8              
8      eltwise_10_output                     1088/1088   activations/**default**   s8                    
9      eltwise_11_output                     1088/1088   activations/**default**   s8                    
10     eltwise_2_output                      1216/1216   activations/**default**   s8                    
11     eltwise_3_output                      1216/1216   activations/**default**   s8                    
12     gemm_14_bias                          64/256      weights/weights           const s32             
13     gemm_14_output                        64/64       activations/**default**   s8                    
14     gemm_14_scratch0                      384/768     activations/**default**   s16                   
15     gemm_14_weights                       4096/4096   weights/weights           const s8              
16     gemm_15_bias                          5/20        weights/weights           const s32             
17     gemm_15_output                        5/5         activations/**default**   s8                    
18     gemm_15_scratch0                      89/178      activations/**default**   s16                   
19     gemm_15_weights                       320/320     weights/weights           const s8              
20     nl_16_output                          5/5         activations/**default**   s8          /output   
21     nl_16_scratch0                        124/496     activations/**default**   s32                   
22     pool_13_output                        64/64       activations/**default**   s8                    
23     pool_6_output                         608/608     activations/**default**   s8                    
24     serving_default_input_layer0_output   240/240     activations/**default**   s8          /input    
25     tfl_pseudo_qconst4_4D                 64/64       weights/weights           const s8              
26     tfl_pseudo_qconst5_4D                 64/64       weights/weights           const s8              
27     tfl_pseudo_qconst8_4D                 32/32       weights/weights           const s8              
28     tfl_pseudo_qconst9_4D                 32/32       weights/weights           const s8              
------ ------------------------------------- ----------- ------------------------- ----------- --------- 

C-Layers (11)
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
c_id   name (*_layer)   id   layer_type      macc     rom    tensors                                  shape (array id)       
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
0      conv2d_1         1    Conv2D          21920    704    I: serving_default_input_layer0_output   int8(1x40x6) (24)      
                                                             S: conv2d_1_scratch0                                            
                                                             W: conv2d_1_weights                      int8(32x1x3x6) (3)     
                                                             W: conv2d_1_bias                         int32(32) (0)          
                                                             O: conv2d_1_output                       int8(1x1x38x32) (1)    
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
1      eltwise_2        2    Eltwise/mul     1216     32     I: conv2d_1_output                       int8(1x1x38x32) (1)    
                                                             W: tfl_pseudo_qconst9_4D                 int8(1x1x32) (28)      
                                                             O: eltwise_2_output                      int8(1x1x38x32) (10)   
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
2      eltwise_3        3    Eltwise/add     1216     32     I: eltwise_2_output                      int8(1x1x38x32) (10)   
                                                             W: tfl_pseudo_qconst8_4D                 int8(1x1x32) (27)      
                                                             O: eltwise_3_output                      int8(1x1x38x32) (11)   
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
3      pool_6           6    Pool            1216     0      I: eltwise_3_output                      int8(1x1x38x32) (11)   
                                                             O: pool_6_output                         int8(1x1x19x32) (23)   
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
4      conv2d_9         9    Conv2D          104512   6400   I: pool_6_output                         int8(1x1x19x32) (23)   
                                                             S: conv2d_9_scratch0                                            
                                                             W: conv2d_9_weights                      int8(64x1x3x32) (7)    
                                                             W: conv2d_9_bias                         int32(64) (4)          
                                                             O: conv2d_9_output                       int8(1x1x17x64) (5)    
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
5      eltwise_10       10   Eltwise/mul     1088     64     I: conv2d_9_output                       int8(1x1x17x64) (5)    
                                                             W: tfl_pseudo_qconst5_4D                 int8(1x1x64) (26)      
                                                             O: eltwise_10_output                     int8(1x1x17x64) (8)    
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
6      eltwise_11       11   Eltwise/add     1088     64     I: eltwise_10_output                     int8(1x1x17x64) (8)    
                                                             W: tfl_pseudo_qconst4_4D                 int8(1x1x64) (25)      
                                                             O: eltwise_11_output                     int8(1x1x17x64) (9)    
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
7      pool_13          13   Pool            1088     0      I: eltwise_11_output                     int8(1x1x17x64) (9)    
                                                             O: pool_13_output                        int8(1x1x64) (22)      
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
8      gemm_14          14   Dense           4160     4352   I: pool_13_output                        int8(1x1x64) (22)      
                                                             S: gemm_14_scratch0                                             
                                                             W: gemm_14_weights                       int8(64x64) (15)       
                                                             W: gemm_14_bias                          int32(64) (12)         
                                                             O: gemm_14_output                        int8(1x64) (13)        
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
9      gemm_15          15   Dense           325      340    I: gemm_14_output                        int8(1x64) (13)        
                                                             S: gemm_15_scratch0                                             
                                                             W: gemm_15_weights                       int8(5x64) (19)        
                                                             W: gemm_15_bias                          int32(5) (16)          
                                                             O: gemm_15_output                        int8(1x5) (17)         
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 
10     nl_16            16   Nonlinearity    75       0      I: gemm_15_output                        int8(1x5) (17)         
                                                             S: nl_16_scratch0                                               
                                                             O: nl_16_output                          int8(1x5) (20)         
------ ---------------- ---- --------------- -------- ------ ---------------------------------------- ---------------------- 



Number of operations per c-layer
------- ------ -------------------------- --------- ------------ 
c_id    m_id   name (type)                      #op         type 
------- ------ -------------------------- --------- ------------ 
0       1      conv2d_1 (Conv2D)             21,920   smul_s8_s8 
1       2      eltwise_2 (Eltwise/mul)        1,216     op_s8_s8 
2       3      eltwise_3 (Eltwise/add)        1,216     op_s8_s8 
3       6      pool_6 (Pool)                  1,216   smul_s8_s8 
4       9      conv2d_9 (Conv2D)            104,512   smul_s8_s8 
5       10     eltwise_10 (Eltwise/mul)       1,088     op_s8_s8 
6       11     eltwise_11 (Eltwise/add)       1,088     op_s8_s8 
7       13     pool_13 (Pool)                 1,088   smul_s8_s8 
8       14     gemm_14 (Dense)                4,160   smul_s8_s8 
9       15     gemm_15 (Dense)                  325   smul_s8_s8 
10      16     nl_16 (Nonlinearity)              75     op_s8_s8 
------- ------ -------------------------- --------- ------------ 
total                                       137,904 

Number of operation types
---------------- --------- ----------- 
operation type           #           % 
---------------- --------- ----------- 
smul_s8_s8         133,221       96.6% 
op_s8_s8             4,683        3.4% 

Complexity report (model)
------ -------------------- ------------------------- ------------------------- ------ 
m_id   name                 c_macc                    c_rom                     c_id   
------ -------------------- ------------------------- ------------------------- ------ 
1      conv2d_1             ||||              15.9%   ||                 5.9%   [0]    
2      tfl_pseudo_qconst9   |                  0.9%   |                  0.3%   [1]    
3      tfl_pseudo_qconst8   |                  0.9%   |                  0.3%   [2]    
6      pool_6               |                  0.9%   |                  0.0%   [3]    
9      conv2d_9             ||||||||||||||||  75.8%   ||||||||||||||||  53.4%   [4]    
10     tfl_pseudo_qconst5   |                  0.8%   |                  0.5%   [5]    
11     tfl_pseudo_qconst4   |                  0.8%   |                  0.5%   [6]    
13     pool_13              |                  0.8%   |                  0.0%   [7]    
14     tfl_pseudo_qconst3   |                  3.0%   |||||||||||       36.3%   [8]    
15     tfl_pseudo_qconst1   |                  0.2%   |                  2.8%   [9]    
16     nl_16                |                  0.1%   |                  0.0%   [10]   
------ -------------------- ------------------------- ------------------------- ------ 
macc=137,904 weights=11,988 act=8,096 ram_io=0
 
 Requested memory size by section - "stm32l4" target
 ---------------------------------- -------- -------- ------- ------- 
 module                                 text   rodata    data     bss 
 ---------------------------------- -------- -------- ------- ------- 
 NetworkRuntime1020_CM4_GCC.a         30,220        0       0       0 
 harsh_detection_prototype.o             804    1,341   3,720     228 
 harsh_detection_prototype_data.o         48       16      88       0 
 lib (toolchain)*                      2,120        0       0       0 
 ---------------------------------- -------- -------- ------- ------- 
 RT total**                           33,192    1,357   3,808     228 
 ---------------------------------- -------- -------- ------- ------- 
 weights                                   0   11,992       0       0 
 activations                               0        0       0   8,096 
 io                                        0        0       0       0 
 ---------------------------------- -------- -------- ------- ------- 
 TOTAL                                33,192   13,349   3,808   8,324 
 ---------------------------------- -------- -------- ------- ------- 
 *  toolchain objects (libm/libgcc*)
 ** RT AI runtime objects (kernels+infrastructure)
  
  Summary - "stm32l4" target
  ---------------------------------------------------
               FLASH (ro)      %*   RAM (rw)       % 
  ---------------------------------------------------
  RT total         38,357   76.2%      4,036   33.3% 
  ---------------------------------------------------
  TOTAL            50,349             12,132         
  ---------------------------------------------------
  *  rt/total


Generated files (7)
--------------------------------------------------------------------------------------------------------- 
/home/cyber-surge/.stm32cubemx/harsh_detection_prototype_output/harsh_detection_prototype_data_params.h   
/home/cyber-surge/.stm32cubemx/harsh_detection_prototype_output/harsh_detection_prototype_data_params.c   
/home/cyber-surge/.stm32cubemx/harsh_detection_prototype_output/harsh_detection_prototype_data.h          
/home/cyber-surge/.stm32cubemx/harsh_detection_prototype_output/harsh_detection_prototype_data.c          
/home/cyber-surge/.stm32cubemx/harsh_detection_prototype_output/harsh_detection_prototype_config.h        
/home/cyber-surge/.stm32cubemx/harsh_detection_prototype_output/harsh_detection_prototype.h               
/home/cyber-surge/.stm32cubemx/harsh_detection_prototype_output/harsh_detection_prototype.c               
